{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7a04bd",
   "metadata": {},
   "source": [
    "# 1 Program to scrape data for “Data Analyst” Job position in “Bangalore” location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f4aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf01df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15a2b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Entering designation and location \n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fe06640",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5e2ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "#scraping the job title from the given page\n",
    "job_title.clear()\n",
    "job_location.clear()\n",
    "company_name.clear()\n",
    "experience_required.clear()\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scraping the job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping the company names from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "#scraping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "748aee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reference Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cargill</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst, plsql, Tableau, Informatica</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>Capgemini</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst - SQL/Tableau</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Global Employees</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst - Data Visualization &amp; Reporting</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>Global Employees</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bright Money</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leap COE Intern(Data Analyst)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Info Origin Inc.</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Job_title  \\\n",
       "0                                   Data Analyst   \n",
       "1                            Senior Data Analyst   \n",
       "2                         Reference Data Analyst   \n",
       "3                                   Data Analyst   \n",
       "4      Data Analyst, plsql, Tableau, Informatica   \n",
       "5              Senior Data Analyst - SQL/Tableau   \n",
       "6                         Senior Data Analyst II   \n",
       "7  Data Analyst - Data Visualization & Reporting   \n",
       "8                                   Data Analyst   \n",
       "9                  Leap COE Intern(Data Analyst)   \n",
       "\n",
       "                                        Job_location      Company_name  \\\n",
       "0                                Bangalore/Bengaluru             Bayer   \n",
       "1                                Bangalore/Bengaluru             Optum   \n",
       "2                                Bangalore/Bengaluru     Deutsche Bank   \n",
       "3                                Bangalore/Bengaluru           Cargill   \n",
       "4  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...         Capgemini   \n",
       "5  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...  Global Employees   \n",
       "6                                Bangalore/Bengaluru          Flipkart   \n",
       "7  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...  Global Employees   \n",
       "8                                Bangalore/Bengaluru      Bright Money   \n",
       "9                                Bangalore/Bengaluru  Info Origin Inc.   \n",
       "\n",
       "  Experience_required  \n",
       "0             2-5 Yrs  \n",
       "1             5-7 Yrs  \n",
       "2             2-5 Yrs  \n",
       "3             3-5 Yrs  \n",
       "4             3-8 Yrs  \n",
       "5            7-12 Yrs  \n",
       "6             3-6 Yrs  \n",
       "7            5-10 Yrs  \n",
       "8             0-2 Yrs  \n",
       "9             0-1 Yrs  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))\n",
    "\n",
    "#Creating the dataframe from above data\n",
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name,'Experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1acb5ca",
   "metadata": {},
   "source": [
    "# 2 Program to scrape data for “Data Scientist” Job position in “Bangalore” location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce03fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf4b1707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-3.8.3-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.64.0)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.4)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-0.20.0 webdriver-manager-3.8.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e24616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9cca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d92328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f86f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation and location \n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6987139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71803b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f641eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da531c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BCAI - Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bosch Global Software Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bosch Global Software Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bosch Global Software Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr. Clinical Data Management Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical Data Management Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Full Stack</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad</td>\n",
       "      <td>Salesforce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior/Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>BLJ TECH GEEKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Global Employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>SuccessR Hrtech Pvt Ltd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Job_title  \\\n",
       "0              BCAI - Senior Data Scientist   \n",
       "1                            Data Scientist   \n",
       "2                            Data Scientist   \n",
       "3    Sr. Clinical Data Management Scientist   \n",
       "4        Clinical Data Management Scientist   \n",
       "5               Data Scientist - Full Stack   \n",
       "6                            Data Scientist   \n",
       "7           Senior/Principal Data Scientist   \n",
       "8  Senior Data Scientist - Machine Learning   \n",
       "9                        Sr. Data Scientist   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5        Bangalore/Bengaluru, Hyderabad/Secunderabad   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "9            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "\n",
       "                         Company_name  \n",
       "0  Bosch Global Software Technologies  \n",
       "1  Bosch Global Software Technologies  \n",
       "2  Bosch Global Software Technologies  \n",
       "3                           Accenture  \n",
       "4                           Accenture  \n",
       "5                          Salesforce  \n",
       "6                               Optum  \n",
       "7                      BLJ TECH GEEKS  \n",
       "8                    Global Employees  \n",
       "9             SuccessR Hrtech Pvt Ltd  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job title from the given page\n",
    "job_title.clear()\n",
    "job_location.clear()\n",
    "company_name.clear()\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping the job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping the company names from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "print(len(job_title),len(job_location),len(company_name))\n",
    "\n",
    "#Creating the dataframe from above data\n",
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234eb4d",
   "metadata": {},
   "source": [
    "# 3 Scrape data using the filters available on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84aa0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.8.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a43a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Job_title, Job_location, Company_name, Experience_required]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "#first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#Opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "#Entering designation and location \n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "job_title.clear()\n",
    "job_location.clear()\n",
    "company_name.clear()\n",
    "experience_required.clear()\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scraping the job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "#scraping the company names from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "#scraping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)\n",
    "\n",
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))\n",
    "\n",
    "#Creating the dataframe from above data\n",
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name,'Experience_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f840",
   "metadata": {},
   "source": [
    "# 4 Scrape data of first 100 sunglasses listings on flipkart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "url = 'https://www.flipkart.com'\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    login_pop = driver.find_element_by_class_name('_2KpZ6l _2doB4z')\n",
    "    # Here .click function use to tap on desire elements of webpage\n",
    "    login_pop.click()\n",
    "    print('pop-up closed')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Here I get search field id from driver\n",
    "search_field = driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "# Here .send_keys is use to input text in search field\n",
    "search_field.send_keys('sunglasses' + '\\n')\n",
    "# Here time.sleep is used clsto add delay for loading context in browser\n",
    "time.sleep(5)\n",
    "\n",
    "get_url = driver.current_url\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "for page in range(1, 5, 1):\n",
    "    page_url = get_url + \"&page=\" + str(page)\n",
    "    element_list.append(page_url)\n",
    "\n",
    "brands = []\n",
    "productDescriptionsList = []\n",
    "OriginalPriceList = []\n",
    "DiscountPercentageList = []\n",
    "FinalPriceList = []\n",
    "brands.clear()\n",
    "productDescriptionsList.clear()\n",
    "OriginalPriceList.clear()\n",
    "DiscountPercentageList.clear()\n",
    "FinalPriceList.clear()\n",
    "counter1 = 0\n",
    "counter2= 0\n",
    "counter3 = 0\n",
    "counter4 = 0\n",
    "counter5 = 0\n",
    "for i in element_list:\n",
    "    driver.get(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d3553",
   "metadata": {},
   "source": [
    "# Q 5 Scrape 100 reviews data from flipkart.com for iphone11 phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "    \n",
    "    \n",
    "url = 'https://www.flipkart.com'\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "    print('pop-up closed')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Here I get search field id from driver\n",
    "search_field = driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "# Here .send_keys is use to input text in search field\n",
    "search_field.send_keys('iphone 11' + '\\n')\n",
    "# Here time.sleep is used clsto add delay for loading context in browser\n",
    "time.sleep(5)\n",
    "\n",
    "get_url = driver.current_url\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "for page in range(1, 5, 1):\n",
    "    page_url = get_url + \"&page=\" + str(page)\n",
    "    element_list.append(page_url)\n",
    "  \n",
    "\n",
    "Ratings = []\n",
    "Ratings.clear()\n",
    "reviewSummary = []\n",
    "Ratings.clear()\n",
    "for i in element_list:\n",
    "    driver.get(i)\n",
    "    RatingsList = driver.find_elements(By.XPATH, '//div[@class=\"_3LWZlK\"]')\n",
    "    for j in RatingsList:\n",
    "        Ratings.append(j)\n",
    "\n",
    " \n",
    "    \"\"\" reviewSummaryList = driver.find_elements(By.XPATH, '//span[@class=\"_13vcmD\"]')\n",
    "    reviews = []\n",
    "    while True:\n",
    "        try:\n",
    "            for more_element in WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.XPATH, \"//div[@data-testid='review-item']//following::div[1]//a[text()='(more)']\"))):\n",
    "                more_element.click()\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "        reviews.append([my_elem.text for my_elem in WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.XPATH, \"//div[starts-with(., 'Posted') or starts-with(., 'Updated')]//following::div[1]/div/span\")))])\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//li[@class='page page-active']//following::li[1]/a[@class='page-link']\"))).click()\n",
    "        except TimeoutException:\n",
    "            break\n",
    "        print(reviews) \"\"\"\n",
    "        \n",
    "        \n",
    "get_url = driver.current_url   \n",
    "# print(len(reviewSummary))\n",
    "print(get_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8f810",
   "metadata": {},
   "source": [
    "# Q6 Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "url = 'https://www.flipkart.com'\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    login_pop = driver.find_element_by_class_name('_2KpZ6l _2doB4z')\n",
    "    # Here .click function use to tap on desire elements of webpage\n",
    "    login_pop.click()\n",
    "    print('pop-up closed')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Here I get search field id from driver\n",
    "search_field = driver.find_element(By.CLASS_NAME, \"_3704LK\")\n",
    "# Here .send_keys is use to input text in search field\n",
    "search_field.send_keys('sneakers' + '\\n')\n",
    "# Here time.sleep is used clsto add delay for loading context in browser\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "get_url = driver.current_url\n",
    "\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "element_list.clear()\n",
    "for page in range(1, 5, 1):\n",
    "    page_url = get_url + \"&page=\" + str(page)\n",
    "    element_list.append(page_url)\n",
    "\n",
    "\n",
    "brands = []\n",
    "brands.clear()\n",
    "productDescriptionsList = []\n",
    "productDescriptionsList.clear()\n",
    "FinalPriceList = []\n",
    "FinalPriceList.clear()\n",
    "\n",
    "counter1 = 0\n",
    "counter2= 0\n",
    "counter3 = 0\n",
    "\n",
    "# Iterate through eah page and collect brands, productDescriptionsList and FinalPriceList\n",
    "for i in element_list:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82faa2",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import contains\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from lxml import html\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "url = 'https://www.amazon.in/'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    login_pop = driver.find_element_by_class_name('_2KpZ6l _2doB4z')\n",
    "    # Here .click function use to tap on desire elements of webpage\n",
    "    login_pop.click()\n",
    "    print('pop-up closed')\n",
    "except:\n",
    "    pass\n",
    "# # Here I get search field id from driver\n",
    "# search_field = driver.find_element(By.CLASS_NAME, \"nav-search-submit-text nav-sprite nav-progressive-attribute\")\n",
    "# # Here .send_keys is use to input text in search field\n",
    "# search_field.send_keys('Laptop')\n",
    "\n",
    "url='https://www.amazon.in/s?k=laptop&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031&dc&ds=v1%3AYM6zXdnsHhY6FzU%2BBFLz6q0A%2BLmt%2F7LZY0nOMpmlXnw&qid=1661337866&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_11'\n",
    "driver.get(url)\n",
    "\n",
    "#01. Get the Titles\n",
    "counter1 = 0\n",
    "Titles = []\n",
    "Titles.clear()\n",
    "TitlesList = driver.find_elements(By.XPATH, '//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "\n",
    "for i in TitlesList:\n",
    "    if(counter1 < 10):\n",
    "        Titles.append(i.text)\n",
    "    counter1 = counter1 + 1\n",
    "\n",
    "#02. Get the prices \n",
    "prices = []\n",
    "prices.clear()\n",
    "counter3 = 0\n",
    "pricesList = driver.find_elements(By.XPATH, '//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "for i in pricesList:\n",
    "    if(counter3 < 10):\n",
    "        prices.append(i.text)\n",
    "    counter3 = counter3 + 1\n",
    "\n",
    "#03. Get the ratings \n",
    "ratings = []\n",
    "ratings.clear()\n",
    "counter2 = 0\n",
    "ratingsList = driver.find_elements(By.XPATH, '//div[@class=\"a-row a-size-small\"]')\n",
    "# ratingsList = driver.find_elements(By.XPATH, '//div[@class=\"a-section a-spacing-none a-spacing-top-micro\"]') # wrong output\n",
    "#ratingsList = driver.find_elements(By.XPATH, '//span[@class=\"a-declarative\"]')\n",
    "counter3 = 0\n",
    "for i in ratingsList:\n",
    "    if(counter3 < 10):\n",
    "        ratings.append(i.text)\n",
    "    counter3 = counter3 + 1\n",
    "\n",
    "# #04. Print the values\n",
    "df = pd.DataFrame({'TITLES': Titles, 'PRICE': prices, 'RATINGS': ratings})\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b24350",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3469ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "#url = ' '\n",
    "url = 'https://www.ambitionbox.com/jobs/search?tag=data%20scientist&location=Noida'\n",
    "\n",
    "driver.get(url)\n",
    "get_url = driver.current_url\n",
    "\n",
    "# Here I get search field id from driver\n",
    "# jobsPage = driver.find_element(By.CLASS_NAME, \"navItemLink\")\n",
    "# jobsPage.click()\n",
    "\n",
    "# searchBox = driver.find_element(By.CLASS_NAME, \"twitter-typeahead\")\n",
    "# searchBox.send_keys(\"data scientist\")\n",
    "\n",
    "# buttonObj = driver.find_element(By.CLASS_NAME, \"ab_btn search-btn round\")\n",
    "# buttonObj.click\n",
    "\n",
    "#01. Get the company names and ratings\n",
    "hiringCompaniesList = driver.find_element(By.XPATH, '//div[@class=\"jobsList\"]')\n",
    "companyNames = []\n",
    "ratings = []\n",
    "jobLife = []\n",
    "hiringCompaniesList1 = hiringCompaniesList.find_elements(By.XPATH, '//div[@class=\"company-info\"]')\n",
    "companyNames.clear()\n",
    "for i in hiringCompaniesList1:\n",
    "    companyNames.append(i.text.split(\"\\n\")[0])\n",
    "    ratings.append(i.text.split(\"\\n\")[1])\n",
    "\n",
    "#01. Get the information about number of days to go \n",
    "otherInfo = hiringCompaniesList.find_elements(By.XPATH, '//span[@class=\"body-small-l\"]')\n",
    "counter = 0\n",
    "for i in otherInfo:\n",
    "    if(i.text != 'via naukri.com'  and  i.text != 'via iimjobs.com'):\n",
    "        if(counter < 10):\n",
    "            jobLife.append(i.text)\n",
    "            counter = counter + 1\n",
    "    \n",
    "\n",
    "#03. Print the values\n",
    "df = pd.DataFrame({'COMPANY NAMES': companyNames, 'RATINGS': ratings, 'Number of days Go': jobLife})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407a6c1",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51af538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "    \n",
    "    \n",
    "#url = 'https://www.ambitionbox.com/'\n",
    "#driver.get(url)\n",
    "\n",
    "#01. Get the driver obe=ject\n",
    "driver.get('https://www.ambitionbox.com/profile/data-scientist-salary')\n",
    "\n",
    "\"\"\" elems = driver.find_elements(By.XPATH,'//a[@class=\"subNavListItem\"]')\n",
    "myurlList = []\n",
    "for lnk in elems:\n",
    "    if lnk.get_attribute('href').__contains__(\"salaries?\"):\n",
    "        myurlList.append(lnk.get_attribute('href'))\n",
    "        print(lnk.get_attribute('href'))\n",
    "    \n",
    "driver.get(myurlList[0])\n",
    " \"\"\"\n",
    "\n",
    "#02. Collect the company names\n",
    "companyNames = []\n",
    "companyNames.clear()\n",
    "companyNamesList = driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]')\n",
    "for i in companyNamesList:\n",
    "    startIndex = i.text.find('\\n')\n",
    "    companyNames.append(i.text[0:startIndex].strip())\n",
    "# print(len(companyNames))\n",
    "\n",
    "\n",
    "#03. Collect the information about experience required and total salary records\n",
    "expRequired = []\n",
    "totalSalRec = []\n",
    "expRequired.clear()\n",
    "totalSalRec.clear()\n",
    "counter1 = 0\n",
    "expRequiredList = driver.find_elements(By.XPATH,'//div[@class=\"sbold-list-header\"]')\n",
    "for i in expRequiredList:\n",
    "    if counter1 > 0:\n",
    "        startIndex = i.text.find('(')\n",
    "        endIndex = i.text.find(')')\n",
    "        expRequired.append(i.text[0:startIndex].strip())\n",
    "        totalSalRec.append(i.text[startIndex+1:endIndex].strip())\n",
    "    counter1 = counter1 + 1\n",
    "\n",
    "\n",
    "#04. Collect the details about MAX, MIN and AVERAGE salary\n",
    "minSalary = []\n",
    "maxSalary = []\n",
    "avgSalary = []\n",
    "minSalary.clear()\n",
    "maxSalary.clear()\n",
    "avgSalary.clear()\n",
    "minSalaryList = driver.find_elements(By.XPATH,'//div[@class=\"salary-range-wrapper\"]')\n",
    "\n",
    "salaryComps = []\n",
    "salaryComps.clear()\n",
    "for i in minSalaryList:\n",
    "    salaryComp1 = i.text.split(',')\n",
    "    salaryComp2 = i.text.split(\"\\n\")\n",
    "    salaryComps.append(salaryComp2)\n",
    "\n",
    "#05. Populate values in lists\n",
    "for x in range(9,len(salaryComps)):\n",
    "    myElement2 = salaryComps[x]\n",
    "    avgSalary.append(myElement2[0])\n",
    "    minSalary.append(myElement2[1])\n",
    "    maxSalary.append(myElement2[2])\n",
    "    #print(\"*******\")\n",
    "\n",
    "#06. Print the values\n",
    "df = pd.DataFrame({'COMPANY NAMES': companyNames, 'TOTAL SALARY RECORD': totalSalRec, 'EXPERIENCE REQUIRED': expRequired, 'AVERAGE SALARY': avgSalary,'MINIMUM SALARY': minSalary,'MAXIMUM SALARY': maxSalary})\n",
    "print(df)\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
