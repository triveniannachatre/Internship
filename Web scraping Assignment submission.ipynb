{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0583ee4",
   "metadata": {},
   "source": [
    "# Program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24c729",
   "metadata": {},
   "source": [
    "# Program to display IMDB’s Top rated 100 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading imdb top 100 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movies = soup.select('td.titleColumn')\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value')\n",
    "        for b in soup.select('td.posterColumn span[name=ir]')]\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    " \n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0,100):\n",
    "     \n",
    "    # Separating movie into: 'place',\n",
    "    # 'title', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-len(movie)]\n",
    "    data = {\"place\": place,\n",
    "            \"movie_title\": movie_title,\n",
    "            \"rating\": ratings[index],\n",
    "            \"year\": year,\n",
    "                        }\n",
    "    list.append(data)\n",
    " \n",
    "# printing movie details with its rating.\n",
    "for movie in list:\n",
    "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "        ') -',  movie['rating'])\n",
    " \n",
    " \n",
    "##.......##\n",
    "df = pd.DataFrame(list)\n",
    "df.to_csv('imdb_top_100_movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59d8a0",
   "metadata": {},
   "source": [
    "# Program to display list of respected former presidents of India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63702be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# display list of respected former presidents of India(i.e. Name , Term of office).\n",
    "url ='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect president Names\n",
    "presidentNames = []   \n",
    "for title in soup.find_all('h3'):\n",
    "    presidentNames.append(title.get_text())\n",
    "    #print(title.get_text())\n",
    "\n",
    "# Collect term of office of all prsidents\n",
    "spanList1 = []   \n",
    "for mySpan1 in soup.find_all('span', class_=\"terms\"):\n",
    "    spanList1.append(mySpan1.find_next_sibling(text=True))\n",
    "    #print(mySpan1.find_next_sibling(text=True))\n",
    "\n",
    "# Print all the required data   \n",
    "for ii in range(len(presidentNames)):\n",
    "    print(presidentNames[ii] + \" | \" + spanList1[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2074d6c",
   "metadata": {},
   "source": [
    "# Program to scrape cricket rankings from icc-cricket.com."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445ff71",
   "metadata": {},
   "source": [
    "# Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5072ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "url ='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "myCollection=[] #collection of all the required data as per the tag and class\n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().replace('\\n', '').strip())\n",
    "   \n",
    "\n",
    "for i in range(0, 50, 5):\n",
    "    print(myCollection[i] + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] + \" | \" + myCollection[i + 4])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee24b6",
   "metadata": {},
   "source": [
    "# Top 10 ODI Batsmen along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "url ='https://www.icc-cricket.com//rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "myCollection=[] #collection of all the required data as per the tag and class\n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().strip().replace('\\n','').replace(\"(0)\",'').strip())\n",
    "   \n",
    "#since we are scraping only 4 elements the 5th element will be ignored while printing\n",
    "for i in range(0, 50, 5):\n",
    "    print(myCollection[i] + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5004594",
   "metadata": {},
   "source": [
    "# Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading Top 10 ODI bowlers along with the records of their team and rating.\n",
    "url ='https://www.icc-cricket.com//rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "myCollection=[] \n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().replace('\\n','').replace(\"(0)\",\"\").strip())\n",
    "\n",
    "#myCollection\n",
    "\n",
    "for i in range(0, 50, 5):\n",
    "    print(myCollection[i] + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6404e",
   "metadata": {},
   "source": [
    "# Program to scrape cricket rankings from icc-cricket.com."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b478f6f",
   "metadata": {},
   "source": [
    "# Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeffa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "url ='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "myCollection=[] \n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().replace('\\n', '').strip())\n",
    "   \n",
    "\n",
    "for i in range(0, 50, 5):\n",
    "    print(myCollection[i] + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] + \" | \" + myCollection[i + 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7fbd2",
   "metadata": {},
   "source": [
    "# Top 10 women’s ODI Batting players along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os, socket\n",
    " \n",
    "# Downloading Top 10 women’s ODI Batting players along with the records of their team and rating\n",
    "url ='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "myCollection=[] \n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().replace('\\n','').replace(\"(0)\",\"\").strip())\n",
    "   \n",
    "\n",
    "for i in range(0, 50, 5):\n",
    "        strTobeIgnored1 = \"This player has moved up in the rankings since the previous rankings update\"\n",
    "        strTobeIgnored2 = \"(1)This player has moved down in the rankings since the previous rankings update\"\n",
    "        if strTobeIgnored1 in myCollection[i] or strTobeIgnored2 in myCollection[i]:\n",
    "            myTempVar = myCollection[i].split()[0]\n",
    "            print(myTempVar + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] )\n",
    "        else:\n",
    "            myTempVar = myCollection[i]\n",
    "            print(myTempVar + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8739a",
   "metadata": {},
   "source": [
    "# Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Downloading Top 10 women’s ODI all-rounder along with the records of their team and rating\n",
    "url =\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "myCollection=[] \n",
    "\n",
    "\n",
    "for word in soup.find_all('td'):\n",
    "    myCollection.append(word.get_text().replace('\\n', '').replace('(0)', '').strip())\n",
    "   \n",
    "\n",
    "for i in range(0, 50, 5):\n",
    "    print(myCollection[i] + \" | \" + myCollection[i + 1] + \" | \" + myCollection[i + 2] + \" | \" + myCollection[i + 3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf75207",
   "metadata": {},
   "source": [
    "# Program to scrape mentioned news details from https://www.cnbc.com/world/?region=world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00651c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Program to scrape mentioned news details from https://www.cnbc.com/world/?region=world.\n",
    "url ='https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "myCollection1=[] \n",
    "for i in soup.find_all('li',class_=\"LatestNews-item\"):\n",
    "    myCollection1.append(i.text)\n",
    "\n",
    "myCollection2=[] \n",
    "for i in soup.find_all('span',class_=\"QuickLinks-quickLink\"):\n",
    "    myCollection2.append(i.text)\n",
    "\n",
    "print(\"Time and HeadLines: \")  \n",
    "for i in range(0, len(myCollection1) ):\n",
    "    print(myCollection1[i] )\n",
    "    \n",
    "print(\"\\n\\nQuick Links : \")\n",
    "for i in range(0, len(myCollection2) ):\n",
    "    print(myCollection2[i] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93b32b",
   "metadata": {},
   "source": [
    "# Program to scrape the details of most downloaded articles from AI in last 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "url ='https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect all the titles\n",
    "titles = []\n",
    "#print(\"Title of the website is : \")\n",
    "for title in soup.find_all('h2'):\n",
    "    titles.append(title.get_text())\n",
    "    #print(title.get_text())\n",
    "\n",
    "# Collect all the authors\n",
    "    author = []   \n",
    "for title in soup.find_all('span', class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    author.append(title.get_text())\n",
    "    #print(title.get_text())\n",
    "\n",
    "# Collect all the publishedDates\n",
    "publishedDates = []   \n",
    "for mydate1 in soup.find_all('span', class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    publishedDates.append(mydate1.get_text())\n",
    "    #print(mydate1.get_text())\n",
    "\n",
    "# Collect all the urls\n",
    "urls = []\n",
    "for link in soup.find_all('a', class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    urls.append(link.get('href'))\n",
    "    #print(link.get('href'))\n",
    "\n",
    "# Print all the required data   \n",
    "for ii in range(len(urls)):\n",
    "    print(titles[ii] + \" | \" + author[ii]+ \" | \" + publishedDates[ii]+ \" | \" + urls[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b470337",
   "metadata": {},
   "source": [
    "# Program to scrape mentioned details from dineout.co.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# program to scrape mentioned details from dineout.co.in\n",
    "url = 'https://www.dineout.co.in/bangalore-restaurants'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect all the urls\n",
    "titles = [] #empty list for storing\n",
    "for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "    titles.append(i.text)\n",
    "titles\n",
    "\n",
    "cuisine=[]#empty list for storing\n",
    "for i in soup.find_all('div', class_=\"detail-info\"):\n",
    "    cuisine.append(i.text)\n",
    "cuisine\n",
    "\n",
    "locations=[]#empty list for storing\n",
    "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "    locations.append(i.text)\n",
    "locations\n",
    "\n",
    "ratings=[]#empty list for storing\n",
    "for i in soup.find_all(\"div\", {\"class\": [\"restnt-rating rating-4\", \"restnt-rating rating-5\"]}):\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "\n",
    "images=[]#empty list for storing\n",
    "for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "images\n",
    "\n",
    "#Printing Length\n",
    "print(len(titles),len(cuisine),len(ratings),len(images),len(locations))\n",
    "\n",
    "#making dataframe\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Titles':titles,'Cuisine':cuisine,'Ratings':ratings,'Images':images,'Locations':locations})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae3c33",
   "metadata": {},
   "source": [
    "# Program to scrape the details of top publications from Google Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# program to scrape the details of top publications from Google Scholar\n",
    "url = 'https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect all the ranks\n",
    "ranks = [] #empty list for storing\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_p\"):\n",
    "    ranks.append(i.text)\n",
    "#ranks\n",
    "\n",
    "# Collect all the publications\n",
    "publication = [] #empty list for storing\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "    publication.append(i.text)\n",
    "#publication\n",
    "\n",
    "# Collect all the h5index\n",
    "h5index = [] #empty list for storing\n",
    "for i in soup.find_all('a', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5index.append(i.text)\n",
    "#h5index\n",
    "\n",
    "# Collect all the h5median\n",
    "h5median = [] #empty list for storing\n",
    "for i in soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5median.append(i.text)\n",
    "#h5median\n",
    "\n",
    "#Printing Length\n",
    "print(len(ranks),len(publication),len(h5index),len(h5median))\n",
    "\n",
    "#making dataframe\n",
    "#import pandas as pd\n",
    "#df=pd.DataFrame({'Ranks':ranks,'Publication':publication,'H5index':h5index,'H5median':h5median,})\n",
    "#df\n",
    "\n",
    "for ii in range(len(ranks)):\n",
    "    print(ranks[ii] +\" | \"+publication[ii]+\" | \"+h5index[ii]+\" | \"+h5median[ii])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
